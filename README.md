# LanguageModels
This repository contains a walkthrough fundamental Language Models. The notebook file present inside each folder contains the code and appropriate explanation to the development of the model and the framework used for development.

## 1. MakeMore
This model develops the understanding of sequence based probabilistic learning by walking through the implementation of a bigram language model developed using a probability graph and optimized further using a Multi-Layer perceptron. 

A list of names is used as the input to the model, the model is first trained only on the previous character pattern recognition and further optimized to develop contextual understanding using Multi-Layer Perceptron.

## 2. SongSmith
Application of self attention mechanism as read and understood from the paper *Vaswani et al.’s ’Attention is All You Need’*. Notebook file contains the implementation code with step by step explanation. 

This model contains minor changes from as implemented in the paper, which has been discussed in the notebook.
